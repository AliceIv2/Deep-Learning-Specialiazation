# Week 2

## Optimization Algorithms 

Develop your deep learning toolbox by adding more advanced optimizations, random minibatching, and learning rate decay scheduling to speed up your models.

1. Mini-Batch Gradient Descent.
2. Exponentially Weighted Averages.
3. Bias Correction in Exponentially Weighted Averages.
4. Gradient Descent with Momentum.
5. RMSprop. 
6. Adam Optimization Algorithm. 
7. Learning Rate Decay. 
